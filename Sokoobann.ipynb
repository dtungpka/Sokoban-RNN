{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_sokoban\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from collections import namedtuple,defaultdict,deque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#from torch.autograd import Variable\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pick: xsbs/11/71.xsb, False, d:\\2023-2024\\RNN\\Sokoban\n",
      "Created environment: Sokoban-v1\n"
     ]
    }
   ],
   "source": [
    "env_name = 'Sokoban-v1'\n",
    "env = gym.make(env_name)\n",
    "ACTION_LOOKUP = env.unwrapped.get_action_lookup()\n",
    "print(\"Created environment: {}\".format(env_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pick: xsbs/14/56.xsb, True, d:\\2023-2024\\RNN\\Sokoban\n",
      "Created environment: Sokoban-v1\n",
      "pick: xsbs/0/1.xsb, True, d:\\2023-2024\\RNN\\Sokoban\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [3, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 5, 3, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "ACTION_LOOKUP = env.unwrapped.get_action_lookup()\n",
    "print(\"Created environment: {}\".format(env_name))\n",
    "env.unwrapped.set_level(0,1)\n",
    "env.seed(0)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: (1, 26, 26)\n",
      "action space: Discrete(8)\n"
     ]
    }
   ],
   "source": [
    "print('observation space:', env.observation_space.shape)\n",
    "print('action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in env.unwrapped.room_state:\n",
    "    print(i)\n",
    "    for j in i:\n",
    "        print (j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env):\n",
    "    num_iterations = 10000\n",
    "    threshold = 1e-20\n",
    "    gamma = 1.0\n",
    "    value_table = np.zeros(env.observation_space.shape)\n",
    "    for i in range(num_iterations):\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        for state in range(env.observation_space.n):\n",
    "            Q_value = [sum([p*(r+gamma*updated_value_table[s_]) for p,s_,r,_ in env.P[state][action]]) for action in range(env.action_space.n)]\n",
    "            value_table[state] = max(Q_value)\n",
    "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
    "            break\n",
    "    return value_table\n",
    "def extract_policy(value_table):\n",
    "    gamma = 1.0\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for state in range(env.observation_space.n):\n",
    "        Q_table = [sum([prob*(reward + gamma*value_table[next_state]) for prob,next_state,reward,_ in env.P[state][action]]) for action in range(env.action_space.n)]\n",
    "        policy[state] = np.argmax(np.array(Q_table))\n",
    "    return policy\n",
    "def compute_value_function(policy):\n",
    "    gamma = 1.0\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    threshold = 1e-10\n",
    "    while True:\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        for state in range(env.observation_space.n):\n",
    "            action = policy[state]\n",
    "            value_table[state] = sum([prob*(reward + gamma*updated_value_table[next_state]) for prob,next_state,reward,_ in env.P[state][action]])\n",
    "        if (np.sum((np.fabs(updated_value_table - value_table))) <= threshold):\n",
    "            break\n",
    "    return value_table\n",
    "def policy_iteration(env):\n",
    "    num_iterations = 1000\n",
    "    gamma = 1.0\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for i in range(num_iterations):\n",
    "        value_function = compute_value_function(policy)\n",
    "        updated_policy = extract_policy(value_function)\n",
    "        if (np.all(policy == updated_policy)):\n",
    "            break\n",
    "        policy = updated_policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_value_function = value_iteration(env=env)\n",
    "optimal_policy = extract_policy(optimal_value_function)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = policy_iteration(env=env)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Policy Iteration and Value Iteration to solve\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\2023-2024\\RNN\\Sokoban\\Sokoobann.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m env\u001b[39m.\u001b[39mn \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m env\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mP \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(env\u001b[39m.\u001b[39mn):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m#prob,next_state,reward,done in env.env.P[state][action] \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# [(1.0, 400, -1, False)]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         mapping_cord \u001b[39m=\u001b[39m CHANGE_COORDINATES[action \u001b[39m%\u001b[39m \u001b[39m4\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "CHANGE_COORDINATES = {\n",
    "    0: (-1, 0),\n",
    "    1: (1, 0),\n",
    "    2: (0, -1),\n",
    "    3: (0, 1)\n",
    "}\n",
    "env.n = env.observation_space.shape[1] * env.observation_space.shape[2]\n",
    "env.env.P = []\n",
    "for x,y in range(env.n):\n",
    "    #prob,next_state,reward,done in env.env.P[state][action] \n",
    "    # [(1.0, 400, -1, False)]\n",
    "    for action in range(env.action_space.n):\n",
    "        mapping_cord = CHANGE_COORDINATES[action % 4]\n",
    "        next_state = (x + mapping_cord[0])+ ((y + mapping_cord[1]) * env.env.P.append((1.0, next_state, -1, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.env.nS)\n",
    "    while True:\n",
    "        # TODO: Implement!\n",
    "        delta = 0  #delta = change in value of state from one iteration to next\n",
    "       \n",
    "        for state in range(env.env.nS):  #for all states\n",
    "            val = 0  #initiate value as 0\n",
    "            \n",
    "            for action,act_prob in enumerate(policy[state]): #for all actions/action probabilities\n",
    "                for prob,next_state,reward,done in env.env.P[state][action]:  #transition probabilities,state,rewards of each action\n",
    "                    val += act_prob * prob * (reward + discount_factor * V[next_state])  #eqn to calculate\n",
    "            delta = max(delta, np.abs(val-V[state]))\n",
    "            V[state] = val\n",
    "        if delta < theta:  #break if the change in value is less than the threshold (theta)\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "def policy_iteration(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.env.nA)\n",
    "        for a in range(env.env.nA):\n",
    "            for prob, next_state, reward, done in env.env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA\n",
    "\n",
    "    while True:\n",
    "        # Implement this!\n",
    "        curr_pol_val = policy_eval_fn(policy, env, discount_factor)  #eval current policy\n",
    "        policy_stable = True  #Check if policy did improve (Set it as True first)\n",
    "        for state in range(env.env.nS):  #for each states\n",
    "            chosen_act = np.argmax(policy[state])  #best action (Highest prob) under current policy\n",
    "            act_values = one_step_lookahead(state,curr_pol_val)  #use one step lookahead to find action values\n",
    "            best_act = np.argmax(act_values) #find best action\n",
    "            if chosen_act != best_act:\n",
    "                policy_stable = False  #Greedily find best action\n",
    "            policy[state] = np.eye(env.env.nA)[best_act]  #update \n",
    "        if policy_stable:\n",
    "            return policy, curr_pol_val\n",
    "    \n",
    "    return policy, np.zeros(env.env.nS)\n",
    "\n",
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.env.nA)\n",
    "        for act in range(env.env.nA):\n",
    "            for prob, next_state, reward, done in env.env.P[state][act]:\n",
    "                A[act] += prob * (reward + discount_factor*V[next_state])\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pick: xsbs/0/1.xsb, True, d:\\2023-2024\\RNN\\Sokoban\n",
      "pick: xsbs/0/1.xsb, True, d:\\2023-2024\\RNN\\Sokoban\n",
      "pick: xsbs/0/1.xsb, True, d:\\2023-2024\\RNN\\Sokoban\n",
      "move up 1.9 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push up 1.7999999999999998 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move left 1.6999999999999997 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up 1.5999999999999996 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right 1.4999999999999996 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push down 1.3999999999999995 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right 1.2999999999999994 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move up 1.1999999999999993 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move right 1.0999999999999992 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right 0.9999999999999992 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push up 0.8999999999999992 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down 0.7999999999999993 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move up 0.6999999999999993 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move left 0.5999999999999993 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left 0.49999999999999933 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up 0.39999999999999936 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right 0.2999999999999994 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move left 0.19999999999999937 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move down 0.09999999999999937 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -6.38378239159465e-16 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push up -0.10000000000000064 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': True}\n",
      "push down -0.20000000000000065 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -0.30000000000000066 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move left -0.4000000000000007 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -0.5000000000000007 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -0.6000000000000006 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right -0.7000000000000006 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push down -0.8000000000000006 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -0.9000000000000006 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move right -1.0000000000000007 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -1.1000000000000008 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left -1.2000000000000008 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -1.300000000000001 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -1.400000000000001 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move right -1.500000000000001 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push down -1.6000000000000012 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -1.7000000000000013 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -1.8000000000000014 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move up -1.9000000000000015 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right -2.0000000000000013 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -2.1000000000000014 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -2.2000000000000015 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -2.3000000000000016 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -2.4000000000000017 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -2.5000000000000018 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -2.600000000000002 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -2.700000000000002 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -2.800000000000002 False {'action.name': 'push right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -2.900000000000002 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -3.000000000000002 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -3.1000000000000023 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -3.2000000000000024 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move up -3.3000000000000025 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -3.4000000000000026 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -3.5000000000000027 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -3.6000000000000028 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move right -3.700000000000003 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -3.800000000000003 False {'action.name': 'push right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -3.900000000000003 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -4.000000000000003 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move down -4.100000000000002 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -4.200000000000002 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -4.300000000000002 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move up -4.400000000000001 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -4.500000000000001 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -4.6000000000000005 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -4.7 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push down -4.8 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -4.8999999999999995 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -4.999999999999999 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left -5.099999999999999 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -5.199999999999998 False {'action.name': 'push right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -5.299999999999998 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move up -5.399999999999998 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right -5.499999999999997 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push down -5.599999999999997 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -5.699999999999997 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -5.799999999999996 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -5.899999999999996 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -5.999999999999996 False {'action.name': 'push right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -6.099999999999995 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -6.199999999999995 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -6.2999999999999945 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -6.399999999999994 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -6.499999999999994 False {'action.name': 'push right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -6.599999999999993 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -6.699999999999993 False {'action.name': 'push right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -6.799999999999993 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -6.899999999999992 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -6.999999999999992 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -7.099999999999992 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -7.199999999999991 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -7.299999999999991 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -7.399999999999991 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -7.49999999999999 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move down -7.59999999999999 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -7.6999999999999895 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move down -7.799999999999989 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -7.899999999999989 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -7.9999999999999885 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -8.099999999999989 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -8.199999999999989 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move up -8.299999999999988 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move left -8.399999999999988 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left -8.499999999999988 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left -8.599999999999987 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left -8.699999999999987 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -8.799999999999986 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -8.899999999999986 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move up -8.999999999999986 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -9.099999999999985 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -9.199999999999985 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -9.299999999999985 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -9.399999999999984 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -9.499999999999984 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -9.599999999999984 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -9.699999999999983 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push down -9.799999999999983 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -9.899999999999983 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right -9.999999999999982 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push down -10.099999999999982 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -10.199999999999982 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move right -10.299999999999981 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move down -10.39999999999998 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -10.49999999999998 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left -10.59999999999998 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move down -10.69999999999998 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -10.79999999999998 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -10.899999999999979 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -10.999999999999979 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -11.099999999999978 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -11.199999999999978 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -11.299999999999978 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left -11.399999999999977 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -11.499999999999977 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -11.599999999999977 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push up -11.699999999999976 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -11.799999999999976 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -11.899999999999975 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -11.999999999999975 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -12.099999999999975 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -12.199999999999974 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -12.299999999999974 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left -12.399999999999974 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left -12.499999999999973 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move up -12.599999999999973 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push down -12.699999999999973 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move up -12.799999999999972 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -12.899999999999972 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -12.999999999999972 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move up -13.099999999999971 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move left -13.19999999999997 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -13.29999999999997 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -13.39999999999997 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -13.49999999999997 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -13.59999999999997 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right -13.699999999999969 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move up -13.799999999999969 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move right -13.899999999999968 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -13.999999999999968 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -14.099999999999968 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -14.199999999999967 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -14.299999999999967 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move up -14.399999999999967 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move right -14.499999999999966 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -14.599999999999966 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -14.699999999999966 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -14.799999999999965 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push down -14.899999999999965 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move left -14.999999999999964 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move up -15.099999999999964 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move right -15.199999999999964 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right -15.299999999999963 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push left -15.399999999999963 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -15.499999999999963 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -15.599999999999962 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move down -15.699999999999962 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -15.799999999999962 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move up -15.899999999999961 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -15.999999999999961 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -16.099999999999962 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push right -16.199999999999964 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move left -16.299999999999965 False {'action.name': 'move left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move down -16.399999999999967 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -16.499999999999968 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -16.59999999999997 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -16.69999999999997 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push down -16.799999999999972 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -16.899999999999974 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move up -16.999999999999975 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right -17.099999999999977 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -17.199999999999978 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -17.29999999999998 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -17.39999999999998 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push down -17.499999999999982 False {'action.name': 'push down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -17.599999999999984 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move right -17.699999999999985 False {'action.name': 'move right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right -17.799999999999986 False {'action.name': 'push right', 'action.moved_player': True, 'action.moved_box': False}\n",
      "push right -17.899999999999988 False {'action.name': 'push right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move down -17.99999999999999 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move right -18.09999999999999 False {'action.name': 'move right', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -18.199999999999992 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -18.299999999999994 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push up -18.399999999999995 False {'action.name': 'push up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move up -18.499999999999996 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n",
      "move down -18.599999999999998 False {'action.name': 'move down', 'action.moved_player': False, 'action.moved_box': False}\n",
      "push left -18.7 False {'action.name': 'push left', 'action.moved_player': False, 'action.moved_box': False}\n",
      "move up -18.8 False {'action.name': 'move up', 'action.moved_player': True, 'action.moved_box': False}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\2023-2024\\RNN\\Sokoban\\Sokoobann.ipynb Cell 13\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m last_update \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwhile\u001b[39;00m steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     env\u001b[39m.\u001b[39;49mrender(mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m last_update \u001b[39m<\u001b[39m \u001b[39m.2\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/2023-2024/RNN/Sokoban/Sokoobann.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32md:\\2023-2024\\RNN\\Sokoban\\SokobanEnv\\lib\\site-packages\\gym\\core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\2023-2024\\rnn\\sokoban\\gym-sokoban\\gym_sokoban\\envs\\sokoban_env.py:245\u001b[0m, in \u001b[0;36mSokobanEnv.render\u001b[1;34m(self, mode, close, scale)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m, close\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, scale\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    243\u001b[0m     \u001b[39massert\u001b[39;00m mode \u001b[39min\u001b[39;00m RENDERING_MODES\n\u001b[1;32m--> 245\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_image(mode, scale)\n\u001b[0;32m    247\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m    248\u001b[0m         \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\2023-2024\\rnn\\sokoban\\gym-sokoban\\gym_sokoban\\envs\\sokoban_env.py:265\u001b[0m, in \u001b[0;36mSokobanEnv.get_image\u001b[1;34m(self, mode, scale)\u001b[0m\n\u001b[0;32m    263\u001b[0m     img \u001b[39m=\u001b[39m room_to_tiny_world_rgb(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroom_state, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroom_fixed, scale\u001b[39m=\u001b[39mscale)\n\u001b[0;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     img \u001b[39m=\u001b[39m room_to_rgb(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroom_state, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroom_fixed)\n\u001b[0;32m    267\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\2023-2024\\rnn\\sokoban\\gym-sokoban\\gym_sokoban\\envs\\render_utils.py:59\u001b[0m, in \u001b[0;36mroom_to_rgb\u001b[1;34m(room, room_structure)\u001b[0m\n\u001b[0;32m     57\u001b[0m room \u001b[39m=\u001b[39m room[min_row:max_row\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,min_col:max_col\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     58\u001b[0m \u001b[39m# Assemble the new rgb_room, with all loaded images\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m room_rgb \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros(shape\u001b[39m=\u001b[39;49m(room\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39m128\u001b[39;49m, room\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39m128\u001b[39;49m, \u001b[39m3\u001b[39;49m), dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49muint8)\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(room\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m     61\u001b[0m     x_i \u001b[39m=\u001b[39m i \u001b[39m*\u001b[39m \u001b[39m128\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('Sokoban-v1')\n",
    "env.reset()\n",
    "for i_episode in range(1):#20\n",
    "    observation = env.reset()\n",
    "    steps = 1000\n",
    "    last_update = time.time()\n",
    "    while steps > 0:\n",
    "        env.render(mode='human')\n",
    "        if time.time() - last_update < .2:\n",
    "            continue\n",
    "        last_update = time.time()\n",
    "        steps -= 1\n",
    "        action = env.action_space.sample()\n",
    "        # Sleep makes the actions visible for users\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        print(ACTION_LOOKUP[action], reward, done, info)\n",
    "        if done:\n",
    "            print(\"Episode finished\")\n",
    "            env.render()\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement MDP and temporal difference learning\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q,state, nA,eps):\n",
    "    if np.random.random() > eps:\n",
    "        return np.argmax(Q[state])\n",
    "    else:\n",
    "        return np.random.choice(np.arange(env.action_space.n))\n",
    "    \n",
    "def update_Q_sarsamax(alpha, gamma, Q, state, action, reward, next_state=None):\n",
    "    current = Q[state][action]         # estimate in Q-table (for current state, action pair)\n",
    "    Qsa_next = np.max(Q[next_state]) if next_state is not None else 0  # value of next state \n",
    "    target = reward + (gamma * Qsa_next)               # construct TD target\n",
    "    new_value = current + (alpha * (target - current)) # get updated value \n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env,num_eps, alpha, gamma=1.0,plot_interval=200):\n",
    "    nA = env.action_space.n\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "\n",
    "    tmp_scores = deque(maxlen=plot_interval)\n",
    "    avg_scores = deque(maxlen=num_eps)\n",
    "    for i_eps in range(1,num_eps+1):\n",
    "        if i_eps % plot_interval == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_eps, num_eps), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        eps = 1.0 / i_eps\n",
    "        action = epsilon_greedy(Q,state, nA,eps)\n",
    "        while True:\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            if not done:\n",
    "                next_action = epsilon_greedy(Q,next_state, nA,eps)\n",
    "                Q[state][action] = update_Q_sarsamax(alpha, gamma, Q, \\\n",
    "                                                     state, action, reward, next_state)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            if done:\n",
    "                Q[state][action] = update_Q_sarsamax(alpha, gamma, Q, \\\n",
    "                                                     state, action, reward)\n",
    "                tmp_scores.append(score)\n",
    "                break\n",
    "        if (i_eps % plot_interval == 0):\n",
    "            avg_scores.append(np.mean(tmp_scores))\n",
    "    plt.plot(np.linspace(0,num_eps,len(avg_scores),endpoint=False),np.asarray(avg_scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_interval)\n",
    "    plt.show()\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_interval), np.max(avg_scores))\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo menthod\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pick: xsbs/0/1.xsb, True, d:\\2023-2024\\RNN\\Sokoban\n",
      "Created environment: Sokoban-v1\n",
      "pick: xsbs/0/1.xsb, True, d:\\2023-2024\\RNN\\Sokoban\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "ACTION_LOOKUP = env.unwrapped.get_action_lookup()\n",
    "print(\"Created environment: {}\".format(env_name))\n",
    "env.unwrapped.set_level(0,1)\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(V,x,y):\n",
    "    #.9 prob of greedy action\n",
    "    #.1 prob of random action\n",
    "    r_choice = np.choose([0,1],[.1,.9])\n",
    "    if np.random.random() < r_choice:\n",
    "        return np.random.choice(np.arange(env.action_space.n))\n",
    "    else:\n",
    "        return np.argmax(V[x,y])\n",
    "def find_player(observation):\n",
    "    for x in range(observation.shape[1]):\n",
    "        for y in range(observation.shape[2]):\n",
    "            if observation[0,x,y] == 1:\n",
    "                return x,y\n",
    "    return -1,-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 200\n",
    "gamma = .9\n",
    "obs = (env.observation_space.shape[1],env.observation_space.shape[2],len(ACTION_LOOKUP))\n",
    "V = np.zeros(obs) + 1\n",
    "Returns = defaultdict(list)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    env = gym.make(env_name)\n",
    "    _ = env.reset()\n",
    "    x,y = find_player(_)\n",
    "    for i in range(1000):\n",
    "        action = greedy_policy(V,x,y)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        V[x,y,action] = gamma*V[x,y,action] + reward\n",
    "\n",
    "\n",
    "\n",
    "        x,y = find_player(observation)\n",
    "        print(ACTION_LOOKUP[action], reward, done, info)\n",
    "        if done:\n",
    "            print(\"Episode finished\")\n",
    "            env.render()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SokobanEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
